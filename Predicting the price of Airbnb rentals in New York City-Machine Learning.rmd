```{r}
library(tidyverse)
library(tidyr)
library(dplyr)
library(caret)
library(ggplot2)
library(caTools)
library(ROCR)
library(rpart)
library(rpart.plot)
library(Metrics)
library(ISLR)
library(ipred)
library(randomForest)
library(gbm)
library(e1071)
library(ranger)
library(vtreat)
library(xgboost)
library(lightgbm)




data = read.csv('KaggleData4.0.csv', stringsAsFactors = T)
data2 = data 
data2 = subset(data2, select=-c(price))
scoringData4 = read.csv('scoringData4.0.csv')

```
```{r}
#LINEAR REGRESSION MODEL
#set.seed(617)
#model = lm(price~room_type+guests_included+review_scores_accuracy+minimum_nights+review_scores_rating+bedrooms+bathrooms+availability_90+accommodates+cleaning_fee,data)


#RANDOMFOREST

#set.seed(617)
#forest = randomForest(price~guests_included+review_range+bedrooms+bathrooms+beds+accommodates+minimum_nights+availability_365+cleaning_fee+cancellation_policy,data=data,
             # ntree=50)

#RANGER FOREST
#set.seed(617)
#forest_ranger = ranger(price~guests_included+neighbourhood_group_cleansed+review_range+bedrooms+bathrooms+beds+accommodates+minimum_nights+availability_365+cleaning_fee+cancellation_policy,data=data,
              #num.trees = 500)

#TUNNED FOREST
set.seed(617)
trControl=trainControl(method="cv",number=5)
tuneGrid = expand.grid(mtry=1:3, 
                       splitrule = c('variance','extratrees','maxstat'), 
                       min.node.size = c(5,10,15,20,25))
cvModel = train(price~guests_included+neighbourhood_group_cleansed+review_range+bedrooms+bathrooms+accommodates+minimum_nights+cleaning_fee+hos1_is_superhos1+property_type+room_type,
                data=data,
                method="ranger",
                num.trees=1000,
                trControl=trControl,
                tuneGrid=tuneGrid )
```
```{r}
cv_forest_ranger = ranger(price~guests_included+neighbourhood_group_cleansed+review_range+bedrooms+bathrooms+beds+accommodates+minimum_nights+availability_365+cleaning_fee+cancellation_policy,
                          data=data,
                          num.trees = 1000, 
                          mtry=cvModel$bestTune$mtry, 
                          min.node.size = cvModel$bestTune$min.node.size, 
                          splitrule = cvModel$bestTune$splitrule)

```

```{r}
#submission files for random forest

#pred = predict(cv_forest_ranger,data=scoringData,num.trees = 1000)
#predforest = predict(model,newdata=scoringData)
#submissionFile = data.frame(id = scoringData$id,price=pred$predictions)
#submission_forest = data.frame(id = scoringData$id, price = predforest)
#write.csv(submissionFile, 'submission11.csv',row.names = F)
```
```{r}
#Setting up my test and train inputs for Extreme Gradient Boosting (XGB)
trt1 = designTreatmentsZ(dframe = data2,varlist = names(data2)[1:37])
trt2 = designTreatmentsZ(dframe = scoringData4,varlist = names(scoringData4)[1:37])
newvars1 = trt1$scoreFrame[trt1$scoreFrame$code%in% c('clean','lev'),'varName']
newvars2 = trt2$scoreFrame[trt2$scoreFrame$code%in% c('clean','lev'),'varName']
train_input = prepare(treatmentplan = trt1, 
                      dframe = data2,
                      varRestriction = newvars1)
test_input = prepare(treatmentplan = trt2, 
                     dframe = scoringData4,
                     varRestriction = newvars2)

```

```{r}
param <- list(objective = "reg:squarederror",
              eta=0.1, #Learning rate
              booster="gbtree", #We are using a tree model
              max_depth=c(6,7,8,9), #maximum depth of a tree
              min_child_weight=c(1,2), #Used to control over fitting
              subsample=0.6, #Fraction of observations to be randomly sampled for each tree
              eval_metric="rmse" #Metric to be used for validation data
              ) 
                


```
```{r}
param2 <- list(objective = "reg:squarederror",
              eta=0.1,
              booster="gbtree",
                max_depth = sample(6:10, 1),
                eta = runif(1, .01, .1),   # Learning rate, default: 0.3
                subsample = runif(1, .6, .9),
                colsample_bytree = runif(1, .5, .8), 
                min_child_weight = sample(5:10, 1), # These two are important
                max_delta_step = sample(5:10, 1) )   # Can help to focus error
                                                    # into a small range.


```

```{r}
set.seed(617)
tune_nrounds = xgb.cv(data=as.matrix(train_input), 
                      label = data$price,
                      nrounds=1000,
                      nfold = 5,
                      verbose = 0,
                      param=param) 

```
```{r}
ggplot(data=tune_nrounds$evaluation_log, aes(x=iter, y=test_rmse_mean))+
  geom_point(size=0.4, color='sienna')+
  geom_line(size=0.1, alpha=0.1)+
  theme_bw()
which.min(tune_nrounds$evaluation_log$test_rmse_mean)
```
```{r}
xgboost2= xgboost(data=as.matrix(train_input), 
                  label = data$price,
                  nrounds=1000,
                  verbose = T,
                  param=param,
                  early_stopping_rounds = 20)
```
```{r}
pred1 = predict(xgboost2, 
               newdata=as.matrix(test_input))

```
```{r}
predicted_values=data.frame(scoringData4$id ,pred1)
colnames(predicted_values)=c('id','price')

```
```{r}
write.csv(x = predicted_values, file = "submissionafter4.csv", row.names = FALSE)


```